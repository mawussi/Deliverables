\relax 
\citation{addh09}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}}
\newlabel{sec.introduction}{{1}{3}}
\citation{mice13}
\citation{ptf14}
\citation{ansel:pact:2014}
\@writefile{toc}{\contentsline {section}{\numberline {2}Review of existing software}{4}}
\newlabel{sec.review}{{2}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Applying OpenTuner within NLAFET}{5}}
\newlabel{sec:opentuner}{{3}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Curve fitting}{6}}
\newlabel{sec.curvefitting}{{4}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The optimal block size for DGEMM is independent of the matrix size so a constant model is most appropriate. On this architecture the constant is $304$.}}{6}}
\newlabel{fig.fit_const}{{1}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The optimal block size for DGETRF appears to depend linearly on the matrix size. In this model the intercept is $92.25$ with a gradient of $0.008$.}}{7}}
\newlabel{fig.fit_linear}{{2}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The optimal block size for SPOTRF appears to depend logarithmically on the matrix size. The least-squares fit provides the block size function $y = -898 + 135\qopname  \relax o{log}(x)$.}}{7}}
\newlabel{fig.fit_log}{{3}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The optimal block size for DPOTRF appears to be a step function. The three constants in this step function are $124$, $291$, and $487$.}}{8}}
\newlabel{fig.fit_step}{{4}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Performance results}{8}}
\newlabel{sec.performance}{{5}{8}}
\bibstyle{plain}
\bibdata{/home/srelton/MATFUN/Total_Bibliography.bib}
\bibcite{addh09}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Lua code implementing the curves fitted to the single and double precision Cholesky factorizations. The arguments to the function are \texttt  {dtype}, a character which determines the precisions of the computation, and \texttt  {n}, the size of the matrix.}}{9}}
\newlabel{fig.lua_code}{{5}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{9}}
\newlabel{sec.conclusions}{{6}{9}}
\bibcite{ansel:pact:2014}{2}
\bibcite{mice13}{3}
\bibcite{ptf14}{4}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Tuned and untuned performance for the GEMM kernel. Single precision is on the left and double precision on the right.  }}{10}}
\newlabel{fig.tuned_gemm}{{6}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Tuned and untuned performance for Cholesky factorization. Single precision is on the left and double precision on the right.  }}{10}}
\newlabel{fig.tuned_potrf}{{7}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Tuned and untuned performance for $LU$ factorization. Single precision is on the left and double precision on the right.  }}{11}}
\newlabel{fig.tuned_getrf}{{8}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Tuned and untuned performance for $QR$ factorization. Single precision is on the left and double precision on the right.  }}{11}}
\newlabel{fig.tuned_geqrf}{{9}{11}}
\newlabel{LastPage}{{}{11}}
\xdef\lastpage@lastpage{11}
\gdef\lastpage@lastpageHy{}
