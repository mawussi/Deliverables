\section{Band reduction to bidiagonal form}\label{sec:bidiag}
In this section we briefly discuss the second stage of the reduction
to bidiagonal form.
In fact the standard bidiagonalization
algorithm as proposed by Golub and Kahan requires $\frac{8}{3}n^3$
flops to reduce an $n \times n$ full matrix to a bidiagonal form.
Similarly, the reduction to block bidiagonal form with a
block size of $nb$ (using $nb \times nb$ tiles) performs
$\frac{8}{3} n\times (n-nb)^2$ flops while the second stage
(moving from block bidiagonal to bidiagonal)
requires only $4 n^2\times nb$ flops~\cite{ltaief2013high}.
Since $nb$ is very small compared to $n$,
the first stage is the dominant part of the algorithm.
However since the second stage is memory-bound it can penalize the
overall performance if is not implemented efficiently.

We identify two potential solutions for the second stage.
The first consists of using an existing LAPACK routine:
while LAPACK does not provide any routine to reduce a
full matrix to band bidiagonal form,
the GBBRD routine available in LAPACK is designed to
reduce a band matrix to bidiagonal form.
Relying on a vendor optimized GEBBRD kernel
(for instance Intel MKL) this can be a reasonable solution.

On the other hand, we can implement our own state-of-the-art
solution for the second stage.
In particular we can revisit the cache-aware
and task coalescing techniques introduced by
Haidar~et~al\@.~in~\cite{haidar2011parallel}
which seems to have a good potential for high performance.
We are currently working on an implementation of this
algorithm in OpenMP,
after which we can assess its efficiency by
comparing against the optimized MKL GBBRD kernel.
